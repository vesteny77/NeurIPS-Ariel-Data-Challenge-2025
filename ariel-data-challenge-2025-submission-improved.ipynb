{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ariel Data Challenge 2025 - Advanced Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --no-index --find-links=/kaggle/input/ariel-2024-pqdm pqdm > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import multiprocessing as mp\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import glob\n",
    "from dataclasses import dataclass\n",
    "from typing import Tuple, Dict, Any, Optional, List\n",
    "\n",
    "from tqdm import tqdm\n",
    "from pqdm.threads import pqdm\n",
    "from astropy.stats import sigma_clip\n",
    "from scipy.optimize import minimize, curve_fit\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.signal import savgol_filter\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy import interpolate\n",
    "from scipy.ndimage import median_filter\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Master toggle - set False to use original pipeline\n",
    "USE_ADVANCED_PIPELINE = True\n",
    "\n",
    "ROOT_PATH = \"/kaggle/input/ariel-data-challenge-2025\"\n",
    "MODE = \"test\"\n",
    "EPS = 1e-12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original Config (preserved for compatibility)\n",
    "class Config:\n",
    "    DATA_PATH = '/kaggle/input/ariel-data-challenge-2025'\n",
    "    DATASET = \"test\"\n",
    "\n",
    "    SCALE = 0.946\n",
    "    SIGMA = 0.00056\n",
    "    \n",
    "    CUT_INF = 39\n",
    "    CUT_SUP = 321\n",
    "    \n",
    "    SENSOR_CONFIG = {\n",
    "        \"AIRS-CH0\": {\n",
    "            \"raw_shape\": [11250, 32, 356],\n",
    "            \"calibrated_shape\": [1, 32, CUT_SUP - CUT_INF],\n",
    "            \"linear_corr_shape\": (6, 32, 356),\n",
    "            \"dt_pattern\": (0.1, 4.5), \n",
    "            \"binning\": 30\n",
    "        },\n",
    "        \"FGS1\": {\n",
    "            \"raw_shape\": [135000, 32, 32],\n",
    "            \"calibrated_shape\": [1, 32, 32],\n",
    "            \"linear_corr_shape\": (6, 32, 32),\n",
    "            \"dt_pattern\": (0.1, 0.1),\n",
    "            \"binning\": 30 * 12\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    MODEL_PHASE_DETECTION_SLICE = slice(30, 140)\n",
    "    MODEL_OPTIMIZATION_DELTA = 11\n",
    "    MODEL_POLYNOMIAL_DEGREE = 3\n",
    "    \n",
    "    N_JOBS = 3\n",
    "\n",
    "# Advanced Config\n",
    "@dataclass\n",
    "class AdvancedConfig:\n",
    "    # Calibration / binning\n",
    "    time_bin_factor_fgs: int = 360\n",
    "    time_bin_factor_airs: int = 30\n",
    "    \n",
    "    # Extraction\n",
    "    fgs_aperture_radius_px: int = 5\n",
    "    bg_annulus: Tuple[int, int] = (7, 10)\n",
    "    \n",
    "    # GP/systematics\n",
    "    gp_kernel: str = \"matern32\"\n",
    "    gp_use_shared_hyperparams: bool = True\n",
    "    gp_white_jitter: float = 1e-6\n",
    "    \n",
    "    # Sigma calibration\n",
    "    beta_bin_sizes: Tuple[int, int] = (1, 8)\n",
    "    sigma_floor_fgs: float = 2e-4\n",
    "    sigma_floor_airs: float = 2e-4\n",
    "    \n",
    "    # Wavelength smoothing\n",
    "    wl_smooth_strength: float = 0.05\n",
    "    \n",
    "    # Multi-visit fusion\n",
    "    fuse_visits: bool = True\n",
    "    \n",
    "    # Data paths\n",
    "    data_path: str = '/kaggle/input/ariel-data-challenge-2025'\n",
    "    dataset: str = \"test\"\n",
    "    n_jobs: int = 3\n",
    "\n",
    "config = Config()\n",
    "adv_config = AdvancedConfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original Pipeline Components (Preserved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def _phase_detector_signal(signal, cfg):\n    sl = cfg.MODEL_PHASE_DETECTION_SLICE\n    min_idx = int(np.argmin(signal[sl])) + sl.start\n    s1 = signal[:min_idx]; s2 = signal[min_idx:]\n    \n    if s1.size < 3 or s2.size < 3:\n        return 0, len(signal) - 1\n    \n    g1 = np.gradient(s1); g1_max = np.max(g1) if np.size(g1) else 0.0\n    g2 = np.gradient(s2); g2_max = np.max(g2) if np.size(g2) else 0.0\n    \n    if g1_max != 0:\n        g1 /= g1_max\n    if g2_max != 0:\n        g2 /= g2_max\n    \n    phase1 = int(np.argmin(g1))\n    phase2 = int(np.argmax(g2)) + min_idx\n    \n    return phase1, phase2\n\ndef estimate_sigma_fgs(preprocessed_data, cfg):\n    sig_rel = []\n    delta = cfg.MODEL_OPTIMIZATION_DELTA\n    eps = 1e-12\n    \n    for single in preprocessed_data:\n        air_white = savgol_filter(single[:, 1:].mean(axis=1), 20, 2, mode='nearest')\n        p1, p2 = _phase_detector_signal(air_white, cfg)\n        p1 = max(delta, p1)\n        p2 = min(len(air_white) - delta - 1, p2)\n\n        fgs = single[:, 0]\n        oot = (fgs[: p1 - delta] if p1 - delta > 0 else np.empty(0, fgs.dtype))\n        if p2 + delta < fgs.size:\n            oot = np.concatenate([oot, fgs[p2 + delta :]])\n        inn = fgs[p1 + delta : max(p1 + delta, p2 - delta)]\n\n        if oot.size == 0 or inn.size == 0:\n            sig_rel.append(np.nan); continue\n\n        n_oot, n_in = len(oot), len(inn)\n        var_oot = np.nanvar(oot, ddof=1)\n        var_in  = np.nanvar(inn, ddof=1)\n        oot_mean = float(np.nanmean(oot)) if np.isfinite(np.nanmean(oot)) else float(np.nanmean(fgs))\n        sigma_rel = np.sqrt(var_oot / max(n_oot,1) + var_in / max(n_in,1)) / max(oot_mean, eps)\n        sig_rel.append(sigma_rel)\n\n    s = np.asarray(sig_rel, dtype=float)\n    mask = np.isfinite(s) & (s > 0)\n    med = float(np.nanmedian(s[mask])) if mask.any() else 1.0\n\n    k = np.ones_like(s)\n    if med > 0 and np.isfinite(med):\n        k[mask] = np.sqrt(s[mask] / med)\n    \n    k = np.clip(k, 0.85, 1.30) \n    \n    return k * cfg.SIGMA * 1.04\n\ndef estimate_sigma_air(preprocessed_data, cfg):\n    sig_rel = []\n    delta = cfg.MODEL_OPTIMIZATION_DELTA\n    eps = 1e-12\n\n    for single in preprocessed_data:\n        white = np.nanmean(single[:, 1:], axis=1)\n        white_s = savgol_filter(white, 20, 2, mode='nearest')\n\n        p1, p2 = _phase_detector_signal(white_s, cfg)\n        p1 = max(delta, p1)\n        p2 = min(len(white) - delta - 1, p2)\n\n        oot_left = white[: p1 - delta] if p1 - delta > 0 else np.empty(0, white.dtype)\n        oot_right = white[p2 + delta :] if (p2 + delta) < white.size else np.empty(0, white.dtype)\n        oot = np.concatenate([oot_left, oot_right]) if (oot_left.size + oot_right.size) else oot_left\n        inn = white[p1 + delta : max(p1 + delta, p2 - delta)]\n\n        if oot.size == 0 or inn.size == 0:\n            sig_rel.append(np.nan); continue\n\n        n_oot, n_in = len(oot), len(inn)\n        var_oot = np.nanvar(oot, ddof=1)\n        var_in  = np.nanvar(inn, ddof=1)\n        oot_mean = float(np.nanmean(oot)) if np.isfinite(np.nanmean(oot)) else float(np.nanmean(white))\n\n        sigma_rel = np.sqrt(var_oot / max(n_oot,1) + var_in / max(n_in,1)) / max(oot_mean, eps)\n        sig_rel.append(sigma_rel)\n\n    s = np.asarray(sig_rel, dtype=float)\n    mask = np.isfinite(s) & (s > 0)\n    med = float(np.nanmedian(s[mask])) if mask.any() else 1.0\n\n    k = np.ones_like(s)\n    if med > 0 and np.isfinite(med):\n        k[mask] = np.sqrt(s[mask] / med)\n    \n    k = np.clip(k, 0.92, 1.22)\n\n    return k * cfg.SIGMA * 1.04"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SignalProcessor:\n",
    "    def __init__(self, config):\n",
    "        self.cfg = config\n",
    "        self.adc_info = pd.read_csv(f\"{self.cfg.DATA_PATH}/adc_info.csv\")\n",
    "        self.planet_ids = pd.read_csv(f'{self.cfg.DATA_PATH}/{self.cfg.DATASET}_star_info.csv', index_col='planet_id').index.astype(int)\n",
    "\n",
    "    def _apply_linear_corr(self, linear_corr, signal):\n",
    "        coeffs = np.flip(linear_corr, axis=0)\n",
    "        x = signal.astype(np.float64, copy=False)\n",
    "        out = np.empty_like(x, dtype=np.float64)\n",
    "        out[...] = coeffs[0]\n",
    "        for k in range(1, coeffs.shape[0]):\n",
    "            np.multiply(out, x, out=out)\n",
    "            out += coeffs[k]\n",
    "\n",
    "        return out.astype(signal.dtype, copy=False)\n",
    "\n",
    "    def _calibrate_single_signal(self, planet_id, sensor):\n",
    "        sensor_cfg = self.cfg.SENSOR_CONFIG[sensor]\n",
    "\n",
    "        signal = pd.read_parquet(f\"{self.cfg.DATA_PATH}/{self.cfg.DATASET}/{planet_id}/{sensor}_signal_0.parquet\").to_numpy()\n",
    "        dark = pd.read_parquet(f\"{self.cfg.DATA_PATH}/{self.cfg.DATASET}/{planet_id}/{sensor}_calibration_0/dark.parquet\").to_numpy()\n",
    "        dead = pd.read_parquet(f\"{self.cfg.DATA_PATH}/{self.cfg.DATASET}/{planet_id}/{sensor}_calibration_0/dead.parquet\").to_numpy()\n",
    "        flat = pd.read_parquet(f\"{self.cfg.DATA_PATH}/{self.cfg.DATASET}/{planet_id}/{sensor}_calibration_0/flat.parquet\").to_numpy()\n",
    "        linear_corr = pd.read_parquet(f\"{self.cfg.DATA_PATH}/{self.cfg.DATASET}/{planet_id}/{sensor}_calibration_0/linear_corr.parquet\").values.astype(np.float64).reshape(sensor_cfg[\"linear_corr_shape\"])\n",
    "\n",
    "        signal = signal.reshape(sensor_cfg[\"raw_shape\"])\n",
    "        gain = self.adc_info[f\"{sensor}_adc_gain\"].iloc[0]\n",
    "        offset = self.adc_info[f\"{sensor}_adc_offset\"].iloc[0]\n",
    "        signal = signal / gain + offset\n",
    "\n",
    "        hot = sigma_clip(dark, sigma=5, maxiters=5).mask\n",
    "\n",
    "        if sensor == \"AIRS-CH0\":\n",
    "            signal = signal[:, :, self.cfg.CUT_INF : self.cfg.CUT_SUP]\n",
    "            linear_corr = linear_corr[:, :, self.cfg.CUT_INF : self.cfg.CUT_SUP]\n",
    "            dark = dark[:, self.cfg.CUT_INF : self.cfg.CUT_SUP]\n",
    "            dead = dead[:, self.cfg.CUT_INF : self.cfg.CUT_SUP]\n",
    "            flat = flat[:, self.cfg.CUT_INF : self.cfg.CUT_SUP]\n",
    "            hot = hot[:, self.cfg.CUT_INF : self.cfg.CUT_SUP]\n",
    "\n",
    "\n",
    "        if sensor == \"FGS1\":\n",
    "            y0, y1, x0, x1 = 10, 22, 10, 22\n",
    "            signal = signal[:, y0:y1, x0:x1]\n",
    "            dark   = dark[y0:y1, x0:x1]\n",
    "            dead   = dead[y0:y1, x0:x1]\n",
    "            flat   = flat[y0:y1, x0:x1]\n",
    "            linear_corr = linear_corr[:, y0:y1, x0:x1]\n",
    "            hot    = hot[y0:y1, x0:x1]\n",
    "\n",
    "        np.maximum(signal, 0, out=signal)\n",
    "\n",
    "        if sensor == \"FGS1\":\n",
    "            signal = self._apply_linear_corr(linear_corr, signal)\n",
    "        elif sensor == \"AIRS-CH0\":\n",
    "            sl = (slice(None), slice(10, 22), slice(None))\n",
    "            signal[sl] = self._apply_linear_corr(linear_corr[:, 10:22, :], signal[sl])\n",
    "        else:\n",
    "            signal = self._apply_linear_corr(linear_corr, signal)\n",
    "\n",
    "        base_dt, increment = sensor_cfg[\"dt_pattern\"]\n",
    "        even_scale = base_dt\n",
    "        odd_scale  = base_dt + increment\n",
    "\n",
    "        signal[::2] -= dark * even_scale\n",
    "        signal[1::2] -= dark * odd_scale\n",
    "\n",
    "        \n",
    "        return signal\n",
    "\n",
    "    def _preprocess_calibrated_signal(self, calibrated_signal, sensor):\n",
    "        sensor_cfg = self.cfg.SENSOR_CONFIG[sensor]\n",
    "        binning = sensor_cfg[\"binning\"]\n",
    "\n",
    "        if sensor == \"AIRS-CH0\":\n",
    "            signal_roi = calibrated_signal[:, 10:22, :]\n",
    "        elif sensor == \"FGS1\":\n",
    "            signal_roi = calibrated_signal[:, 10:22, 10:22]\n",
    "            signal_roi = signal_roi.reshape(signal_roi.shape[0], -1)\n",
    "        \n",
    "        mean_signal = np.nanmean(signal_roi, axis=1)\n",
    "\n",
    "        cds_signal = mean_signal[1::2] - mean_signal[0::2]\n",
    "\n",
    "        n_bins = cds_signal.shape[0] // binning\n",
    "        binned = np.array([\n",
    "            cds_signal[j*binning : (j+1)*binning].mean(axis=0) \n",
    "            for j in range(n_bins)\n",
    "        ])\n",
    "\n",
    "        if sensor == \"AIRS-CH0\":\n",
    "            q_lo = np.nanpercentile(binned, 5.0, axis=1, keepdims=True)\n",
    "            q_hi = np.nanpercentile(binned, 95.0, axis=1, keepdims=True)\n",
    "            np.clip(binned, q_lo, q_hi, out=binned)\n",
    "\n",
    "        if sensor == \"FGS1\":\n",
    "            binned = binned.reshape((binned.shape[0], 1))\n",
    "\n",
    "        if sensor == \"AIRS-CH0\":\n",
    "            var = np.nanvar(binned, axis=0, ddof=1)\n",
    "            med = np.nanmedian(var)\n",
    "\n",
    "            safe_var = np.where(~np.isfinite(var) | (var <= 0), med if (np.isfinite(med) and med > 0) else 1.0, var)\n",
    "            w = 1.0 / safe_var\n",
    "\n",
    "            lo, hi = np.nanpercentile(w, 5.0), np.nanpercentile(w, 95.0)\n",
    "            if np.isfinite(lo) and np.isfinite(hi) and lo < hi:\n",
    "                w = np.clip(w, lo, hi)\n",
    "\n",
    "            M = binned.shape[1]\n",
    "            s = np.nansum(w)\n",
    "            if np.isfinite(s) and s > 0:\n",
    "                w = w * (M / s)\n",
    "            else:\n",
    "                w = np.ones_like(w)\n",
    "\n",
    "            binned *= w[None, :]\n",
    "\n",
    "\n",
    "        return binned\n",
    "\n",
    "    def _process_planet_sensor(self, args):\n",
    "        planet_id, sensor = args['planet_id'], args['sensor']\n",
    "        calibrated = self._calibrate_single_signal(planet_id, sensor)\n",
    "        preprocessed = self._preprocess_calibrated_signal(calibrated, sensor)\n",
    "        return preprocessed\n",
    "\n",
    "    def process_all_data(self):\n",
    "        args_fgs1 = [dict(planet_id=planet_id, sensor=\"FGS1\") for planet_id in self.planet_ids]\n",
    "        preprocessed_fgs1 = pqdm(args_fgs1, self._process_planet_sensor, n_jobs=self.cfg.N_JOBS)\n",
    "\n",
    "        args_airs_ch0 = [dict(planet_id=planet_id, sensor=\"AIRS-CH0\") for planet_id in self.planet_ids]\n",
    "        preprocessed_airs_ch0 = pqdm(args_airs_ch0, self._process_planet_sensor, n_jobs=self.cfg.N_JOBS)\n",
    "\n",
    "        preprocessed_signal = np.concatenate(\n",
    "            [np.stack(preprocessed_fgs1), np.stack(preprocessed_airs_ch0)], axis=2\n",
    "        )\n",
    "        \n",
    "        return preprocessed_signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class TransitModel:\n    def __init__(self, config):\n        self.cfg = config\n\n    def _phase_detector(self, signal):\n        search_slice = self.cfg.MODEL_PHASE_DETECTION_SLICE\n        min_index = np.argmin(signal[search_slice]) + search_slice.start\n        \n        signal1 = signal[:min_index]\n        signal2 = signal[min_index:]\n\n        grad1 = np.gradient(signal1)\n        grad1 /= grad1.max()\n        \n        grad2 = np.gradient(signal2)\n        grad2 /= grad2.max()\n\n        phase1 = np.argmin(grad1)\n        phase2 = np.argmax(grad2) + min_index\n\n        return phase1, phase2\n    \n    def _objective_function(self, s, signal, phase1, phase2):\n        delta = self.cfg.MODEL_OPTIMIZATION_DELTA\n        power = self.cfg.MODEL_POLYNOMIAL_DEGREE\n\n        if phase1 - delta <= 0 or phase2 + delta >= len(signal) or phase2 - delta - (phase1 + delta) < 5:\n            delta = 2\n\n        y = np.concatenate([\n            signal[: phase1 - delta],\n            signal[phase1 + delta : phase2 - delta] * (1 + s),\n            signal[phase2 + delta :]\n        ])\n        x = np.arange(len(y))\n\n        coeffs = np.polyfit(x, y, deg=power)\n        poly = np.poly1d(coeffs)\n        error = np.abs(poly(x) - y).mean()\n        \n        return error\n\n    def predict(self, single_preprocessed_signal):\n        signal_1d = single_preprocessed_signal[:, 1:].mean(axis=1)\n        signal_1d = savgol_filter(signal_1d, 20, 2, mode='nearest')\n        \n        phase1, phase2 = self._phase_detector(signal_1d)\n\n        phase1 = max(self.cfg.MODEL_OPTIMIZATION_DELTA, phase1)\n        phase2 = min(len(signal_1d) - self.cfg.MODEL_OPTIMIZATION_DELTA - 1, phase2)    \n\n        result = minimize(\n            fun=self._objective_function,\n            x0=[0.0001],\n            args=(signal_1d, phase1, phase2),\n            method=\"Nelder-Mead\"\n        )\n        \n        return result.x[0]\n\n    def predict_all(self, preprocessed_signals):\n        predictions = [\n            self.predict(preprocessed_signal)\n            for preprocessed_signal in tqdm(preprocessed_signals)\n        ]\n        return np.array(predictions) * self.cfg.SCALE"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Pipeline Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions\n",
    "def robust_mean(x, axis=None):\n",
    "    return np.nanmean(x, axis=axis)\n",
    "\n",
    "def robust_std(x, axis=None):\n",
    "    return np.nanstd(x, axis=axis, ddof=1) + EPS\n",
    "\n",
    "def clip_sigma(s, s_min, s_max=None):\n",
    "    s = np.maximum(s, s_min)\n",
    "    if s_max is not None:\n",
    "        s = np.minimum(s, s_max)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedCalibrator:\n",
    "    \"\"\"Advanced calibration with full pipeline\"\"\"\n",
    "    \n",
    "    def __init__(self, config: AdvancedConfig):\n",
    "        self.cfg = config\n",
    "        self.adc_info = pd.read_csv(f\"{self.cfg.data_path}/adc_info.csv\")\n",
    "        \n",
    "    def load_calibration_frames(self, planet_id, sensor, visit=0):\n",
    "        base_path = f\"{self.cfg.data_path}/{self.cfg.dataset}/{planet_id}/{sensor}_calibration_{visit}\"\n",
    "        \n",
    "        dark = pd.read_parquet(f\"{base_path}/dark.parquet\").to_numpy()\n",
    "        dead = pd.read_parquet(f\"{base_path}/dead.parquet\").to_numpy()\n",
    "        flat = pd.read_parquet(f\"{base_path}/flat.parquet\").to_numpy()\n",
    "        linear_corr = pd.read_parquet(f\"{base_path}/linear_corr.parquet\").to_numpy()\n",
    "        read_noise = pd.read_parquet(f\"{base_path}/read.parquet\").to_numpy()\n",
    "        \n",
    "        return {\n",
    "            'dark': dark,\n",
    "            'dead': dead,\n",
    "            'flat': flat,\n",
    "            'linear_corr': linear_corr,\n",
    "            'read_noise': read_noise\n",
    "        }\n",
    "    \n",
    "    def apply_adc_conversion(self, signal, sensor):\n",
    "        \"\"\"Convert from uint16 to float64 with ADC parameters\"\"\"\n",
    "        gain = self.adc_info[f\"{sensor}_adc_gain\"].iloc[0]\n",
    "        offset = self.adc_info[f\"{sensor}_adc_offset\"].iloc[0]\n",
    "        return signal.astype(np.float64) / gain + offset\n",
    "    \n",
    "    def apply_nonlinearity_correction(self, signal, linear_corr):\n",
    "        \"\"\"Apply polynomial linearity correction\"\"\"\n",
    "        coeffs = np.flip(linear_corr, axis=0)\n",
    "        x = signal.astype(np.float64)\n",
    "        result = np.zeros_like(x)\n",
    "        result[...] = coeffs[0]\n",
    "        \n",
    "        for k in range(1, coeffs.shape[0]):\n",
    "            result *= x\n",
    "            result += coeffs[k]\n",
    "            \n",
    "        return result\n",
    "    \n",
    "    def mask_bad_pixels(self, signal, dead, hot_mask):\n",
    "        \"\"\"Mask and interpolate dead/hot pixels\"\"\"\n",
    "        bad_mask = dead | hot_mask\n",
    "        \n",
    "        if bad_mask.any():\n",
    "            # For 2D images, use median of neighbors\n",
    "            if signal.ndim == 3:\n",
    "                for t in range(signal.shape[0]):\n",
    "                    frame = signal[t]\n",
    "                    if bad_mask.any():\n",
    "                        frame[bad_mask] = median_filter(frame, size=3)[bad_mask]\n",
    "            \n",
    "        return signal\n",
    "    \n",
    "    def apply_flat_fielding(self, signal, flat):\n",
    "        \"\"\"Apply flat field correction\"\"\"\n",
    "        flat_safe = np.where(flat > 0.5, flat, 1.0)\n",
    "        return signal / flat_safe\n",
    "    \n",
    "    def calibrate_signal(self, planet_id, sensor, visit=0):\n",
    "        \"\"\"Full calibration pipeline\"\"\"\n",
    "\n",
    "        # Load raw signal\n",
    "        signal_path = f\"{self.cfg.data_path}/{self.cfg.dataset}/{planet_id}/{sensor}_signal_{visit}.parquet\"\n",
    "        signal = pd.read_parquet(signal_path).to_numpy()\n",
    "\n",
    "        # Reshape based on sensor\n",
    "        if sensor == \"FGS1\":\n",
    "            signal = signal.reshape(135000, 32, 32)\n",
    "        else:  # AIRS-CH0\n",
    "            signal = signal.reshape(11250, 32, 356)\n",
    "\n",
    "        # Load calibration frames\n",
    "        cal_frames = self.load_calibration_frames(planet_id, sensor, visit)\n",
    "\n",
    "        # ADC conversion\n",
    "        signal = self.apply_adc_conversion(signal, sensor)\n",
    "\n",
    "        # CROP EVERYTHING FIRST (matching original pipeline)\n",
    "        if sensor == \"FGS1\":\n",
    "            # Crop signal and all calibration frames to 12x12 region\n",
    "            signal = signal[:, 10:22, 10:22]\n",
    "            cal_frames['dark'] = cal_frames['dark'][10:22, 10:22]\n",
    "            cal_frames['dead'] = cal_frames['dead'][10:22, 10:22]\n",
    "            cal_frames['flat'] = cal_frames['flat'][10:22, 10:22]\n",
    "            # Reshape and crop linear correction\n",
    "            linear_corr = cal_frames['linear_corr'].reshape(6, 32, 32)[:, 10:22, 10:22]\n",
    "        elif sensor == \"AIRS-CH0\":\n",
    "            # Crop to valid spectral range\n",
    "            signal = signal[:, :, 39:321]\n",
    "            cal_frames['dark'] = cal_frames['dark'][:, 39:321]\n",
    "            cal_frames['dead'] = cal_frames['dead'][:, 39:321]\n",
    "            cal_frames['flat'] = cal_frames['flat'][:, 39:321]\n",
    "            # Reshape and crop linear correction\n",
    "            linear_corr = cal_frames['linear_corr'].reshape(6, 32, 356)[:, :, 39:321]\n",
    "\n",
    "        # Ensure signal is non-negative after ADC\n",
    "        np.maximum(signal, 0, out=signal)\n",
    "\n",
    "        # Apply non-linearity correction\n",
    "        if sensor == \"FGS1\":\n",
    "            signal = self.apply_nonlinearity_correction(signal, linear_corr)\n",
    "        elif sensor == \"AIRS-CH0\":\n",
    "            # Only apply to rows 10:22 as in original\n",
    "            sl = (slice(None), slice(10, 22), slice(None))\n",
    "            signal[sl] = self.apply_nonlinearity_correction(signal[sl], linear_corr[:, 10:22, :])\n",
    "\n",
    "        # Dark subtraction with proper scaling\n",
    "        if sensor == \"FGS1\":\n",
    "            dt = 0.1\n",
    "            signal -= cal_frames['dark'] * dt\n",
    "        else:\n",
    "            dt_even = 0.1\n",
    "            dt_odd = 4.6\n",
    "            signal[::2] -= cal_frames['dark'] * dt_even\n",
    "            signal[1::2] -= cal_frames['dark'] * dt_odd\n",
    "\n",
    "        # Hot pixel detection (on cropped dark)\n",
    "        hot_mask = sigma_clip(cal_frames['dark'], sigma=5, maxiters=5).mask\n",
    "\n",
    "        # Mask bad pixels\n",
    "        signal = self.mask_bad_pixels(signal, cal_frames['dead'], hot_mask)\n",
    "\n",
    "        # Flat fielding\n",
    "        signal = self.apply_flat_fielding(signal, cal_frames['flat'])\n",
    "\n",
    "        return signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedExtractor:\n",
    "    \"\"\"Optimal extraction for both instruments\"\"\"\n",
    "    \n",
    "    def __init__(self, config: AdvancedConfig):\n",
    "        self.cfg = config\n",
    "    \n",
    "    def cds_and_bin(self, frames, bin_factor):\n",
    "        \"\"\"Correlated Double Sampling and temporal binning\"\"\"\n",
    "        # CDS: difference consecutive pairs\n",
    "        cds = frames[1::2] - frames[0::2]\n",
    "        \n",
    "        # Temporal binning\n",
    "        n_bins = len(cds) // bin_factor\n",
    "        binned = np.zeros((n_bins,) + cds.shape[1:])\n",
    "        \n",
    "        for i in range(n_bins):\n",
    "            binned[i] = np.mean(cds[i*bin_factor:(i+1)*bin_factor], axis=0)\n",
    "            \n",
    "        return binned\n",
    "    \n",
    "    def centroid(self, image):\n",
    "        \"\"\"Calculate flux-weighted centroid\"\"\"\n",
    "        y, x = np.mgrid[:image.shape[0], :image.shape[1]]\n",
    "        \n",
    "        total = np.sum(image)\n",
    "        if total == 0:\n",
    "            return image.shape[0] // 2, image.shape[1] // 2\n",
    "        \n",
    "        cx = np.sum(x * image) / total\n",
    "        cy = np.sum(y * image) / total\n",
    "        \n",
    "        return cy, cx\n",
    "    \n",
    "    def fgs_aperture_photometry(self, frames, radius=5, bg_annulus=(7, 10)):\n",
    "        \"\"\"Optimal aperture photometry for FGS1\"\"\"\n",
    "        n_frames = frames.shape[0]\n",
    "        photometry = np.zeros(n_frames)\n",
    "        \n",
    "        for i in range(n_frames):\n",
    "            frame = frames[i]\n",
    "            \n",
    "            # Find centroid\n",
    "            cy, cx = self.centroid(frame)\n",
    "            \n",
    "            # Create aperture mask\n",
    "            y, x = np.mgrid[:frame.shape[0], :frame.shape[1]]\n",
    "            dist = np.sqrt((x - cx)**2 + (y - cy)**2)\n",
    "            \n",
    "            # Aperture mask\n",
    "            aperture = dist <= radius\n",
    "            \n",
    "            # Background annulus mask\n",
    "            bg_mask = (dist >= bg_annulus[0]) & (dist <= bg_annulus[1])\n",
    "            \n",
    "            # Calculate background\n",
    "            if bg_mask.any():\n",
    "                bg_median = np.median(frame[bg_mask])\n",
    "            else:\n",
    "                bg_median = 0\n",
    "            \n",
    "            # Aperture photometry\n",
    "            photometry[i] = np.sum(frame[aperture] - bg_median)\n",
    "        \n",
    "        return photometry\n",
    "    \n",
    "    def track_airs_trace(self, frames):\n",
    "        \"\"\"Track spectral trace for AIRS\"\"\"\n",
    "        n_frames, n_rows, n_cols = frames.shape\n",
    "        trace_centers = np.zeros((n_frames, n_cols))\n",
    "        \n",
    "        # Use median frame as reference\n",
    "        ref_frame = np.median(frames, axis=0)\n",
    "        \n",
    "        for col in range(n_cols):\n",
    "            col_profile = ref_frame[:, col]\n",
    "            \n",
    "            # Find peak row (trace center)\n",
    "            if np.sum(col_profile) > 0:\n",
    "                center = np.sum(np.arange(n_rows) * col_profile) / np.sum(col_profile)\n",
    "            else:\n",
    "                center = n_rows // 2\n",
    "            \n",
    "            trace_centers[:, col] = center\n",
    "        \n",
    "        return trace_centers\n",
    "    \n",
    "    def horne_optimal_extraction(self, frames, trace_centers=None, width=5):\n",
    "        \"\"\"Horne optimal extraction for AIRS\"\"\"\n",
    "        n_frames, n_rows, n_cols = frames.shape\n",
    "        \n",
    "        if trace_centers is None:\n",
    "            trace_centers = self.track_airs_trace(frames)\n",
    "        \n",
    "        extracted = np.zeros((n_frames, n_cols))\n",
    "        \n",
    "        for t in range(n_frames):\n",
    "            for col in range(n_cols):\n",
    "                # Extract around trace center\n",
    "                center = int(trace_centers[t, col])\n",
    "                \n",
    "                # Define extraction window\n",
    "                row_min = max(0, center - width)\n",
    "                row_max = min(n_rows, center + width + 1)\n",
    "                \n",
    "                # Simple sum for now (can be improved with proper weights)\n",
    "                col_data = frames[t, row_min:row_max, col]\n",
    "                \n",
    "                # Background subtraction\n",
    "                bg_rows = np.concatenate([\n",
    "                    np.arange(max(0, row_min-3), row_min),\n",
    "                    np.arange(row_max, min(n_rows, row_max+3))\n",
    "                ])\n",
    "                \n",
    "                if len(bg_rows) > 0:\n",
    "                    bg = np.median(frames[t, bg_rows, col])\n",
    "                else:\n",
    "                    bg = 0\n",
    "                    \n",
    "                extracted[t, col] = np.sum(col_data - bg)\n",
    "        \n",
    "        return extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class CommonModeRemoval:\n    \"\"\"Common mode systematics removal\"\"\"\n    \n    @staticmethod\n    def compute_common_mode(white_light, window=51, poly=3):\n        \"\"\"Compute common mode from white light curve\"\"\"\n        # Smooth the white light curve\n        if len(white_light) > window:\n            smooth = savgol_filter(white_light, window, poly, mode='nearest')\n        else:\n            smooth = white_light\n        \n        # Common mode is the residual pattern\n        common_mode = white_light / (smooth + EPS)\n        \n        return common_mode\n    \n    @staticmethod\n    def apply_common_mode(data, common_mode):\n        \"\"\"Apply common mode correction to data\"\"\"\n        if data.ndim == 1:\n            return data / (common_mode + EPS)\n        else:\n            # For 2D data (time, wavelength)\n            return data / (common_mode[:, None] + EPS)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class TransitFitter:\n    \"\"\"Physics-based transit fitting with GP systematics\"\"\"\n    \n    def __init__(self, config: AdvancedConfig):\n        self.cfg = config\n    \n    def simple_transit_model(self, time, depth, t0, duration):\n        \"\"\"Simple box-car transit model\"\"\"\n        transit = np.ones_like(time)\n        in_transit = np.abs(time - t0) < duration / 2\n        transit[in_transit] = 1.0 - depth\n        return transit\n    \n    def detect_transit_phase(self, light_curve):\n        \"\"\"Detect transit ingress and egress\"\"\"\n        light_curve = np.nan_to_num(light_curve, nan=np.nanmedian(light_curve), posinf=np.nanmedian(light_curve), neginf=np.nanmedian(light_curve))\n\n        # Smooth the light curve\n        if len(light_curve) > 51:\n            smooth = savgol_filter(light_curve, 51, 3, mode='nearest')\n        else:\n            smooth = light_curve\n        \n        # Find minimum (mid-transit)\n        mid_transit = np.argmin(smooth)\n        \n        # Find ingress and egress by gradient\n        grad = np.gradient(smooth)\n        \n        # Ingress: steepest negative gradient before mid-transit\n        if mid_transit > 10:\n            ingress = np.argmin(grad[:mid_transit])\n        else:\n            ingress = 0\n        \n        # Egress: steepest positive gradient after mid-transit  \n        if mid_transit < len(grad) - 10:\n            egress = mid_transit + np.argmax(grad[mid_transit:])\n        else:\n            egress = len(light_curve) - 1\n        \n        return ingress, mid_transit, egress\n    \n    def fit_transit_depth(self, time, flux, star_params=None):\n        \"\"\"Fit transit depth with simple detrending\"\"\"\n        \n        # Ensure flux is finite\n        flux_clean = np.nan_to_num(flux, nan=np.nanmedian(flux), posinf=np.nanmedian(flux), neginf=np.nanmedian(flux))\n        \n        # If flux is all NaN or zero, return default values\n        if np.all(np.isnan(flux)) or np.all(flux_clean == 0):\n            return 0.001, 0.0001  # Default depth and sigma\n        \n        # Detect transit timing\n        ingress, mid_transit, egress = self.detect_transit_phase(flux_clean)\n        \n        # Define in and out of transit\n        oot_mask = (time < ingress - 5) | (time > egress + 5)\n        in_transit_mask = (time >= ingress + 5) & (time <= egress - 5)\n        \n        if not oot_mask.any() or not in_transit_mask.any():\n            # Fallback if phase detection fails\n            n = len(flux_clean)\n            oot_mask = np.zeros(n, dtype=bool)\n            oot_mask[:n//4] = True\n            oot_mask[3*n//4:] = True\n            in_transit_mask = ~oot_mask\n        \n        # Estimate depth from ratio\n        oot_level = np.median(flux_clean[oot_mask]) if oot_mask.any() else np.median(flux_clean)\n        in_transit_level = np.median(flux_clean[in_transit_mask]) if in_transit_mask.any() else np.min(flux_clean)\n        \n        # Ensure levels are finite and positive\n        if not np.isfinite(oot_level) or oot_level <= 0:\n            oot_level = np.nanmedian(flux_clean[flux_clean > 0]) if np.any(flux_clean > 0) else 1.0\n        if not np.isfinite(in_transit_level):\n            in_transit_level = oot_level * 0.99  # Default 1% depth\n        \n        depth = 1.0 - (in_transit_level / oot_level)\n        depth = np.clip(depth, 0, 0.1)  # Reasonable bounds\n        \n        # Ensure depth is finite\n        if not np.isfinite(depth):\n            depth = 0.001  # Default depth\n        \n        # Estimate uncertainty from out-of-transit scatter\n        if oot_mask.any():\n            oot_std = robust_std(flux_clean[oot_mask])\n        else:\n            oot_std = robust_std(flux_clean)\n            \n        # Ensure std is finite and positive\n        if not np.isfinite(oot_std) or oot_std <= 0:\n            oot_std = 0.001 * oot_level  # Default to 0.1% of level\n            \n        n_in = np.sum(in_transit_mask)\n        n_out = np.sum(oot_mask)\n        \n        # Propagate uncertainty\n        if n_in > 0 and n_out > 0:\n            sigma_depth = oot_std * np.sqrt(1.0/n_in + 1.0/n_out) / oot_level\n        else:\n            sigma_depth = oot_std / oot_level\n        \n        # Ensure sigma is finite and positive\n        if not np.isfinite(sigma_depth) or sigma_depth <= 0:\n            sigma_depth = 0.0001  # Default uncertainty\n        \n        return depth, sigma_depth\n    \n    def fit_with_gp(self, time, flux, star_params=None):\n        \"\"\"Fit transit with simple GP-like detrending\"\"\"\n        \n        # Initial depth estimate\n        depth, sigma = self.fit_transit_depth(time, flux, star_params)\n        \n        # Simple iterative detrending\n        for _ in range(3):\n            # Build transit model\n            ingress, mid_transit, egress = self.detect_transit_phase(flux)\n            transit = self.simple_transit_model(time, depth, mid_transit, egress - ingress)\n            \n            # Detrend\n            residuals = flux / (transit + EPS)\n            if len(residuals) > 101:\n                trend = savgol_filter(residuals, 101, 3, mode='nearest')\n            else:\n                trend = np.ones_like(residuals)\n            \n            # Correct flux\n            flux_detrended = flux / (trend + EPS)\n            \n            # Re-fit depth\n            depth, sigma = self.fit_transit_depth(time, flux_detrended, star_params)\n        \n        return depth, sigma"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SigmaCalibrator:\n",
    "    \"\"\"Advanced uncertainty estimation\"\"\"\n",
    "    \n",
    "    def __init__(self, config: AdvancedConfig):\n",
    "        self.cfg = config\n",
    "    \n",
    "    def beta_inflation(self, residuals, bin_sizes=(1, 8)):\n",
    "        \"\"\"Calculate beta factor for red noise\"\"\"\n",
    "        rms_unbinned = robust_std(residuals)\n",
    "        \n",
    "        if len(residuals) < bin_sizes[1] * 10:\n",
    "            return 1.0\n",
    "        \n",
    "        # Bin the residuals\n",
    "        n_bins = len(residuals) // bin_sizes[1]\n",
    "        binned = np.array([np.mean(residuals[i*bin_sizes[1]:(i+1)*bin_sizes[1]]) \n",
    "                          for i in range(n_bins)])\n",
    "        \n",
    "        rms_binned = robust_std(binned)\n",
    "        \n",
    "        # Beta factor\n",
    "        expected_reduction = 1.0 / np.sqrt(bin_sizes[1])\n",
    "        actual_reduction = rms_binned / rms_unbinned\n",
    "        \n",
    "        beta = actual_reduction / expected_reduction\n",
    "        beta = np.clip(beta, 1.0, 10.0)\n",
    "        \n",
    "        return beta\n",
    "    \n",
    "    def calibrate_sigma(self, sigma_raw, flux, residuals=None):\n",
    "        \"\"\"Calibrate uncertainty estimate\"\"\"\n",
    "        \n",
    "        # Apply floor\n",
    "        sigma = clip_sigma(sigma_raw, self.cfg.sigma_floor_airs)\n",
    "        \n",
    "        # Beta inflation if residuals available\n",
    "        if residuals is not None:\n",
    "            beta = self.beta_inflation(residuals)\n",
    "            sigma *= beta\n",
    "        \n",
    "        # SNR-based scaling\n",
    "        snr = np.median(flux) / robust_std(flux)\n",
    "        if snr < 10:\n",
    "            sigma *= 1.5\n",
    "        elif snr > 100:\n",
    "            sigma *= 0.9\n",
    "        \n",
    "        return sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisitFusion:\n",
    "    \"\"\"Multi-visit fusion utilities\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_visits(planet_id, data_path, dataset):\n",
    "        \"\"\"Check for multiple visits\"\"\"\n",
    "        planet_path = f\"{data_path}/{dataset}/{planet_id}\"\n",
    "        \n",
    "        # Look for multiple signal files\n",
    "        fgs_files = glob.glob(f\"{planet_path}/FGS1_signal_*.parquet\")\n",
    "        n_visits = len(fgs_files)\n",
    "        \n",
    "        if n_visits > 1:\n",
    "            return list(range(n_visits))\n",
    "        return [0]\n",
    "    \n",
    "    @staticmethod\n",
    "    def fuse_depths(depths, sigmas):\n",
    "        \"\"\"Inverse variance weighted fusion\"\"\"\n",
    "        depths = np.array(depths)\n",
    "        sigmas = np.array(sigmas)\n",
    "        \n",
    "        # Inverse variance weights\n",
    "        weights = 1.0 / (sigmas**2 + EPS)\n",
    "        \n",
    "        # Weighted mean\n",
    "        fused_depth = np.sum(weights * depths) / np.sum(weights)\n",
    "        \n",
    "        # Combined uncertainty\n",
    "        fused_sigma = np.sqrt(1.0 / np.sum(weights))\n",
    "        \n",
    "        return fused_depth, fused_sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class WavelengthSmoother:\n    \"\"\"Cross-wavelength smoothing\"\"\"\n    \n    @staticmethod\n    def smooth_spectrum(spectrum, strength=0.1):\n        \"\"\"Gentle smoothing across wavelengths\"\"\"\n        if strength == 0:\n            return spectrum\n        \n        # Simple Gaussian smoothing\n        window = int(5 * (1 + strength * 10))\n        if window % 2 == 0:\n            window += 1\n        \n        if len(spectrum) > window:\n            smoothed = savgol_filter(spectrum, window, 3, mode='nearest')\n            # Blend with original\n            return (1 - strength) * spectrum + strength * smoothed\n        \n        return spectrum"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class AdvancedPipeline:\n    \"\"\"Main advanced processing pipeline\"\"\"\n    \n    def __init__(self, config: AdvancedConfig):\n        self.cfg = config\n        self.calibrator = AdvancedCalibrator(config)\n        self.extractor = AdvancedExtractor(config)\n        self.transit_fitter = TransitFitter(config)\n        self.sigma_calibrator = SigmaCalibrator(config)\n        self.star_info = pd.read_csv(f\"{config.data_path}/{config.dataset}_star_info.csv\", \n                                     index_col='planet_id')\n    \n    def process_visit(self, planet_id, visit=0):\n        \"\"\"Process single visit\"\"\"\n        \n        # Get star parameters\n        star_params = self.star_info.loc[planet_id].to_dict()\n        \n        # Calibrate signals\n        fgs_cal = self.calibrator.calibrate_signal(planet_id, \"FGS1\", visit)\n        airs_cal = self.calibrator.calibrate_signal(planet_id, \"AIRS-CH0\", visit)\n        \n        # Apply CDS and binning\n        fgs_cds = self.extractor.cds_and_bin(fgs_cal, self.cfg.time_bin_factor_fgs)\n        airs_cds = self.extractor.cds_and_bin(airs_cal, self.cfg.time_bin_factor_airs)\n        \n        # Extract photometry\n        fgs_flux = self.extractor.fgs_aperture_photometry(fgs_cds, \n                                                          self.cfg.fgs_aperture_radius_px,\n                                                          self.cfg.bg_annulus)\n        \n        # Optimal extraction for AIRS\n        airs_flux = self.extractor.horne_optimal_extraction(airs_cds)\n        \n        # Common mode removal\n        fgs_white = fgs_flux  # FGS is already white light\n        airs_white = np.mean(airs_flux, axis=1)\n        \n        # Use FGS as common mode\n        common_mode = CommonModeRemoval.compute_common_mode(fgs_white)\n        \n        # Apply common mode to AIRS\n        airs_corrected = CommonModeRemoval.apply_common_mode(airs_flux, common_mode)\n        \n        # Fit transits\n        time = np.arange(len(fgs_white))\n        \n        # FGS1 depth\n        depth_fgs, sigma_fgs = self.transit_fitter.fit_with_gp(time, fgs_white, star_params)\n        \n        # AIRS depths (per channel)\n        n_channels = airs_corrected.shape[1]\n        depths_airs = np.zeros(n_channels)\n        sigmas_airs = np.zeros(n_channels)\n        \n        for i in range(n_channels):\n            depths_airs[i], sigmas_airs[i] = self.transit_fitter.fit_with_gp(\n                time, airs_corrected[:, i], star_params\n            )\n        \n        # Calibrate uncertainties\n        sigma_fgs = self.sigma_calibrator.calibrate_sigma(sigma_fgs, fgs_white)\n        \n        for i in range(n_channels):\n            sigmas_airs[i] = self.sigma_calibrator.calibrate_sigma(\n                sigmas_airs[i], airs_corrected[:, i]\n            )\n        \n        # Apply wavelength smoothing to AIRS\n        depths_airs = WavelengthSmoother.smooth_spectrum(depths_airs, \n                                                         self.cfg.wl_smooth_strength)\n        \n        # Validation: ensure all outputs are finite and positive\n        if not np.isfinite(depth_fgs) or depth_fgs <= 0:\n            depth_fgs = 0.001  # Default depth\n        if not np.isfinite(sigma_fgs) or sigma_fgs <= 0:\n            sigma_fgs = 0.0001  # Default sigma\n            \n        # Validate AIRS depths and sigmas\n        for i in range(len(depths_airs)):\n            if not np.isfinite(depths_airs[i]) or depths_airs[i] <= 0:\n                depths_airs[i] = 0.001  # Default depth\n            if not np.isfinite(sigmas_airs[i]) or sigmas_airs[i] <= 0:\n                sigmas_airs[i] = 0.0001  # Default sigma\n        \n        # Final validation - replace any remaining NaN/Inf\n        depth_fgs = np.nan_to_num(depth_fgs, nan=0.001, posinf=0.001, neginf=0.001)\n        sigma_fgs = np.nan_to_num(sigma_fgs, nan=0.0001, posinf=0.0001, neginf=0.0001)\n        depths_airs = np.nan_to_num(depths_airs, nan=0.001, posinf=0.001, neginf=0.001)\n        sigmas_airs = np.nan_to_num(sigmas_airs, nan=0.0001, posinf=0.0001, neginf=0.0001)\n        \n        # Ensure positive values\n        depth_fgs = np.abs(depth_fgs) if depth_fgs != 0 else 0.001\n        sigma_fgs = np.abs(sigma_fgs) if sigma_fgs != 0 else 0.0001\n        depths_airs = np.abs(depths_airs)\n        depths_airs[depths_airs == 0] = 0.001\n        sigmas_airs = np.abs(sigmas_airs)\n        sigmas_airs[sigmas_airs == 0] = 0.0001\n        \n        return depth_fgs, sigma_fgs, depths_airs[:282], sigmas_airs[:282]\n    \n    def process_planet(self, planet_id):\n        \"\"\"Process planet with potential multi-visit fusion\"\"\"\n        \n        # Check for multiple visits\n        visits = VisitFusion.get_visits(planet_id, self.cfg.data_path, self.cfg.dataset)\n        \n        if len(visits) == 1:\n            # Single visit\n            return self.process_visit(planet_id, visits[0])\n        \n        # Multi-visit fusion\n        all_depths_fgs = []\n        all_sigmas_fgs = []\n        all_depths_airs = []\n        all_sigmas_airs = []\n        \n        for visit in visits:\n            d_fgs, s_fgs, d_airs, s_airs = self.process_visit(planet_id, visit)\n            all_depths_fgs.append(d_fgs)\n            all_sigmas_fgs.append(s_fgs)\n            all_depths_airs.append(d_airs)\n            all_sigmas_airs.append(s_airs)\n        \n        # Fuse FGS\n        depth_fgs, sigma_fgs = VisitFusion.fuse_depths(all_depths_fgs, all_sigmas_fgs)\n        \n        # Fuse each AIRS channel\n        n_channels = len(all_depths_airs[0])\n        depths_airs = np.zeros(n_channels)\n        sigmas_airs = np.zeros(n_channels)\n        \n        for i in range(n_channels):\n            channel_depths = [d[i] for d in all_depths_airs]\n            channel_sigmas = [s[i] for s in all_sigmas_airs]\n            depths_airs[i], sigmas_airs[i] = VisitFusion.fuse_depths(channel_depths, \n                                                                     channel_sigmas)\n        \n        # Final validation for fused values\n        depth_fgs = np.nan_to_num(depth_fgs, nan=0.001, posinf=0.001, neginf=0.001)\n        sigma_fgs = np.nan_to_num(sigma_fgs, nan=0.0001, posinf=0.0001, neginf=0.0001)\n        depths_airs = np.nan_to_num(depths_airs, nan=0.001, posinf=0.001, neginf=0.001)\n        sigmas_airs = np.nan_to_num(sigmas_airs, nan=0.0001, posinf=0.0001, neginf=0.0001)\n        \n        # Ensure positive\n        depth_fgs = np.abs(depth_fgs) if depth_fgs != 0 else 0.001\n        sigma_fgs = np.abs(sigma_fgs) if sigma_fgs != 0 else 0.0001\n        depths_airs = np.abs(depths_airs)\n        depths_airs[depths_airs == 0] = 0.001\n        sigmas_airs = np.abs(sigmas_airs)\n        sigmas_airs[sigmas_airs == 0] = 0.0001\n        \n        return depth_fgs, sigma_fgs, depths_airs, sigmas_airs"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, dim, p=0.2):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(dim, dim)\n",
    "        self.bn1 = nn.BatchNorm1d(dim)\n",
    "        self.fc2 = nn.Linear(dim, dim)\n",
    "        self.bn2 = nn.BatchNorm1d(dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.relu(self.bn1(self.fc1(x)))\n",
    "        out = self.dropout(out)\n",
    "        out = self.bn2(self.fc2(out))\n",
    "        return self.relu(out + identity)\n",
    "\n",
    "\n",
    "class ResNetMLP(nn.Module):\n",
    "    def __init__(self, input_dim=3, hidden_dim=32, output_dim=1, num_blocks=3, dropout_rate=0.2):\n",
    "        super().__init__()\n",
    "        self.input_layer = nn.Linear(input_dim, hidden_dim)\n",
    "        self.blocks = nn.Sequential(*[ResidualBlock(hidden_dim, p=dropout_rate) for _ in range(num_blocks)])\n",
    "        self.output_layer = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input_layer(x)\n",
    "        x = self.blocks(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ResNetMLP2(nn.Module):\n",
    "    def __init__(self, input_dim=3, hidden_dim=128, output_dim=282, num_blocks=3, dropout_rate=0.2):\n",
    "        super().__init__()\n",
    "        self.input_layer = nn.Linear(input_dim, hidden_dim)\n",
    "        self.blocks = nn.Sequential(*[ResidualBlock(hidden_dim, p=dropout_rate) for _ in range(num_blocks)])\n",
    "        self.output_layer = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input_layer(x)\n",
    "        x = self.blocks(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class SubmissionGenerator:\n    def __init__(self, config):\n        self.cfg = config\n        self.sample_submission = pd.read_csv(\"/kaggle/input/ariel-data-challenge-2025/sample_submission.csv\", index_col=\"planet_id\")\n\n    def create(self, predictions1, predictions2, predictions, sigma_fgs=None, sigma_air=None):\n        planet_ids = self.sample_submission.index\n        n_mu = self.sample_submission.shape[1] // 2\n\n        preds = np.asarray(predictions, dtype=float).reshape(-1)\n        mu = np.tile(preds.reshape(-1, 1), (1, n_mu))\n        mu = np.clip(mu, 0, None)\n\n        sigmas = np.full_like(mu, self.cfg.SIGMA, dtype=float)\n        if sigma_fgs is not None:\n            sigma_fgs = np.asarray(sigma_fgs, dtype=float).reshape(-1)\n            sigmas[:, 0] = np.clip(sigma_fgs, 1e-6, 0.1)\n        if sigma_air is not None:\n            sigma_air = np.asarray(sigma_air, dtype=float)\n            # Handle both 1D and 2D arrays\n            if sigma_air.ndim == 1:\n                sigma_air = sigma_air.reshape(-1, 1)\n                sigmas[:, 1:] = np.clip(sigma_air, 1e-6, 0.1)\n            else:\n                # sigma_air is already (n_planets, n_channels)\n                sigmas[:, 1:283] = np.clip(sigma_air, 1e-6, 0.1)\n\n        submission_df = pd.DataFrame(\n            np.concatenate([mu, sigmas], axis=1),\n            columns=self.sample_submission.columns,\n            index=planet_ids\n        )\n        submission_df.iloc[:, 1:283] = predictions2\n        submission_df.iloc[:, 0] = predictions1\n\n        submission_df.to_csv(\"submission.csv\")\n        return submission_df"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load star information\n",
    "StarInfo = pd.read_csv(ROOT_PATH + f\"/{MODE}_star_info.csv\")\n",
    "StarInfo[\"planet_id\"] = StarInfo[\"planet_id\"].astype(int)\n",
    "PlanetIds = StarInfo[\"planet_id\"].tolist()\n",
    "StarInfo = StarInfo.set_index(\"planet_id\")\n",
    "\n",
    "print(f\"Processing {len(PlanetIds)} planets...\")\n",
    "print(f\"Using {'ADVANCED' if USE_ADVANCED_PIPELINE else 'ORIGINAL'} pipeline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_ADVANCED_PIPELINE:\n",
    "    # Use advanced pipeline\n",
    "    print(\"Running advanced pipeline...\")\n",
    "    \n",
    "    # Initialize pipeline\n",
    "    pipeline = AdvancedPipeline(adv_config)\n",
    "    \n",
    "    # Process all planets\n",
    "    all_depths_fgs = []\n",
    "    all_sigmas_fgs = []\n",
    "    all_depths_airs = []\n",
    "    all_sigmas_airs = []\n",
    "    \n",
    "    for planet_id in tqdm(PlanetIds, desc=\"Processing planets\"):\n",
    "        depth_fgs, sigma_fgs, depths_airs, sigmas_airs = pipeline.process_planet(planet_id)\n",
    "        \n",
    "        all_depths_fgs.append(depth_fgs)\n",
    "        all_sigmas_fgs.append(sigma_fgs)\n",
    "        all_depths_airs.append(depths_airs)\n",
    "        all_sigmas_airs.append(sigmas_airs)\n",
    "    \n",
    "    # Convert to arrays\n",
    "    predictions1 = np.array(all_depths_fgs).reshape(-1, 1)\n",
    "    predictions2 = np.array(all_depths_airs)\n",
    "    sigma_fgs_vec = np.array(all_sigmas_fgs)\n",
    "    sigma_air_vec = np.array(all_sigmas_airs)\n",
    "    \n",
    "    # Overall predictions (for compatibility)\n",
    "    predictions = np.mean(np.concatenate([predictions1, \n",
    "                                          np.mean(predictions2, axis=1, keepdims=True)], \n",
    "                                          axis=1), axis=1)\n",
    "    \n",
    "else:\n",
    "    # Use original pipeline\n",
    "    print(\"Running original pipeline...\")\n",
    "    \n",
    "    # Process signals\n",
    "    signal_processor = SignalProcessor(config)\n",
    "    preprocessed_data = signal_processor.process_all_data()\n",
    "    \n",
    "    # Transit model predictions\n",
    "    model = TransitModel(config)\n",
    "    predictions = model.predict_all(preprocessed_data)\n",
    "    \n",
    "    # Neural network predictions\n",
    "    predictions_df = pd.DataFrame(predictions, columns=[\"transit_depth\"])\n",
    "    \n",
    "    # Prepare features for NN\n",
    "    input_df = StarInfo.copy()\n",
    "    pred_series = predictions_df[\"transit_depth\"].set_axis(input_df.index, copy=False)\n",
    "    input_df.insert(0, \"transit_depth\", (pred_series * 10000).to_numpy())\n",
    "    \n",
    "    features = [\"transit_depth\", \"Rs\", \"i\"]\n",
    "    X = input_df[features].values.astype(\"float32\")\n",
    "    X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "    \n",
    "    # FGS1 predictions\n",
    "    model_fgs = ResNetMLP(num_blocks=80, dropout_rate=0.2)\n",
    "    model_fgs.load_state_dict(torch.load(\"/kaggle/input/fgs1/pytorch/default/1/best_model.pth\"))\n",
    "    model_fgs.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        predictions1 = model_fgs(X_tensor).numpy() / 10000\n",
    "    \n",
    "    # AIRS predictions\n",
    "    model_airs = ResNetMLP2(num_blocks=80, dropout_rate=0.3)\n",
    "    model_airs.load_state_dict(torch.load(\"/kaggle/input/airs/pytorch/default/1/best_model_airs.pth\"))\n",
    "    model_airs.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        predictions2 = model_airs(X_tensor).numpy() / 10000\n",
    "    \n",
    "    # Estimate uncertainties\n",
    "    sigma_fgs_vec = estimate_sigma_fgs(preprocessed_data, config)\n",
    "    sigma_air_vec = estimate_sigma_air(preprocessed_data, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate submission\n",
    "submission_generator = SubmissionGenerator(config)\n",
    "submission = submission_generator.create(predictions1, predictions2, predictions, \n",
    "                                         sigma_fgs=sigma_fgs_vec, sigma_air=sigma_air_vec)\n",
    "\n",
    "print(\"\\nSubmission shape:\", submission.shape)\n",
    "print(\"First row preview:\")\n",
    "print(submission.iloc[0, :10])  # Show first 10 columns\n",
    "print(\"\\nSubmission saved to submission.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}